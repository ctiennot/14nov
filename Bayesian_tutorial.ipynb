{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNCsfRpZaEZv8D75weoXp7i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ctiennot/14nov/blob/main/Bayesian_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Welcome to this tutorial! During those two hours we'll have the opportunity to:\n",
        "- think about spaceships üöÄ\n",
        "- disover Bayesian methods, mainly hierarchical modelling and logistic regression\n",
        "- use the [Numpyro](https://num.pyro.ai/en/latest/getting_started.html) framework for Bayesian inference in python\n",
        "\n",
        "Let's get started!\n",
        "\n",
        "Credit: some parts of this tutorial are shamelessly inspired by [this great tutorial](https://www.stat.ubc.ca/~bouchard/courses/stat520-sp2021-22/Hierarchical_models.html)."
      ],
      "metadata": {
        "id": "94ail83ouTG_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"center\"><img src=\"https://www.explainxkcd.com/wiki/images/7/78/frequentists_vs_bayesians.png\" width=\"400\"/></p>"
      ],
      "metadata": {
        "id": "YPrCp1NJLbAv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PART 0: Introduction and Setup"
      ],
      "metadata": {
        "id": "tGsplzNFSSto"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "-jAkOfNzbdRQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install numpyro pandas numpy matplotlib arviz"
      ],
      "metadata": {
        "id": "Uvhe9_N2O7yT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from arviz import plot_trace\n",
        "import jax\n",
        "import numpyro\n",
        "import numpyro.distributions as dist\n",
        "from numpyro.infer import MCMC, NUTS\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import List\n",
        "import statsmodels.api as sm\n",
        "\n",
        "import matplotlib_inline\n",
        "matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"
      ],
      "metadata": {
        "id": "3MTx5uSnPIO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Motivation: chasing fake news"
      ],
      "metadata": {
        "id": "WlU_-eLWuUw_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"center\"><img src=\"https://github.com/ctiennot/bayesian_tutorial/blob/main/pictures/bb_screenshot.jpg?raw=true\" width=\"700\"/></p>\n",
        "\n",
        "In this [2023 BBC article](https://www.bbc.com/future/article/20230518-what-are-the-odds-of-a-successful-space-launch) we can read:\n",
        "\n",
        "> Three of the Super Heavy booster's 33 engines did not fire on launch, and more failed as it lifted further into the air. Nearly four minutes after initial lift-off, the rocket started tumbling wildly and was deliberately exploded over the Gulf.\n",
        "\n",
        "Also an empirical estimate of the overall probability of failure is reported:\n",
        "\n",
        "> That works out to around a 4% failure rate ‚Äì one in every 25 launches.\n",
        "\n",
        "Is this true? As rigorous statisticans should we trust this figure without further investigations?"
      ],
      "metadata": {
        "id": "U5hX1k_WbiSV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bayesian 101: what you need to understand for the tutorial"
      ],
      "metadata": {
        "id": "rXjTCSAVfG_3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "But first let's go back to the basics of statistics and bayesian inference. We'll try to re-explain it briefly, feel free to skip this part if you already know about all that.\n",
        "\n",
        "Disclaimer: the topic is broad and it seems pretty hard to understand this stuff in a one morning tutorial (but we'll do our best). If you're really interested consider reading some good books on the matter, for instance:\n",
        "- [Bayesian Statistics the Fun Way](https://www.amazon.com/Bayesian-Statistics-Fun-Will-Kurt/dp/1593279566/) (I haven't read it, but seems like a gentle introduction)\n",
        "- [Bayesian Data Analysis by A.Gelman](https://www.amazon.com/Bayesian-Analysis-Chapman-Statistical-Science/dp/1439840954) (the reference in the field, a bit technical but with many concrete and fascinating examples)"
      ],
      "metadata": {
        "id": "r07l1SKufOAH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can not really go on without a quick reminder about the bayes theorem:"
      ],
      "metadata": {
        "id": "oLlMe7ueNVaP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"center\"><img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*Sj8irBMk53oVKweFttSfGA.jpeg\" width=\"700\"/></p>"
      ],
      "metadata": {
        "id": "pl7vjdcZKmdN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at each term:\n",
        "- the <font color='orange'>likelihood</font> can be thought of as \"given my model and its parameters what are the odds of observing my data\".\n",
        "- The \"belief\" for a model is a set of parameters. So the <font color='orange'>posterior</font> says \"what's the evidence for my belief / model parameters given the data I observe\".\n",
        "- The <font color='orange'>prior</font> is our initial guess about the evidence for the set of parameters before observing the data.\n",
        "\n",
        "So the Bayes formula just tells use <font color='orange'>how to rigorously update our prior knowledge</font> about an event (or a set of parameters for a model) given the data we observe."
      ],
      "metadata": {
        "id": "s23GkNQ09BnU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"center\"><img src=\"https://pbs.twimg.com/media/CE5r1ZMUkAAMWIC.png\" width=\"500\"/></p>"
      ],
      "metadata": {
        "id": "aA-tAXEVMXDf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A simplified way to put it is that:\n",
        "- Frequentist folks care about the optimizing the Likelihood: write a model (with some parameters let's say), compute the probability / \"likelihood\" of the data you observe in your model and find the parameters that give the greatest one.\n",
        "- in the Bayesian framework we will have a prior belief about the parameters of the model and we will update it according to the data we observe"
      ],
      "metadata": {
        "id": "TCkV5ld_N2GV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The way Bayesian does the later is by treating <font color='orange'>**model parameters as random variables**</font>. In comparison, in the frequentist world we consider only pointwise estimates: our estimate is only a single value (we can estimate its uncertainty on the side though, let's not be manicheans)."
      ],
      "metadata": {
        "id": "kSXvw83PTJaL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "An example is worth a thousand words: let's assume I flip a coin 10 times and get 9 heads.\n",
        "\n",
        "\n",
        "\n",
        "- My model is a [Binomial distribution](https://en.wikipedia.org/wiki/Binomial_distribution) with parameter p (the probability of getting \"Head\")\n",
        "- We could write the likelihood and get our frequentist estimator $\\hat \\theta=\\frac {n_{heads}}{n} = \\frac{9}{10} = 0.9$\n",
        "\n",
        "Now if I'm doing it in a Bayesian way, I consider that $\\theta$ is not fixed, $\\theta$ is a random variable:\n",
        "- I choose a prior distribution for it, let's say a [Beta distribution](https://en.wikipedia.org/wiki/Beta_distribution) (it's a natural choice for a parameter between 0 and 1).\n",
        "- let's say I choose a Beta($\\alpha$, $\\beta$) (we'll come back later to the impact of the prior choice)\n"
      ],
      "metadata": {
        "id": "7UB7_jXjUCX9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can write the equations (take a deep breath, everything will be fine):\n",
        "- first the likelihood:\n",
        "\n",
        "$$ p(data|\\theta) = \\binom {n_{heads} + n_{tails}}{n_{heads}} \\theta^{n_{heads}} (1 - \\theta)^{n_{tails}}$$\n",
        "\n",
        "- then the prior distribution (it's the density of a Beta distribution, cf the wikipedia page for instance):\n",
        "\n",
        "$$p(\\theta) = \\frac{\\theta^{\\alpha -1}(1-\\theta)^{\\beta -1}}{\\mathrm{B}(\\alpha ,\\beta )}$$\n",
        "\n",
        "($\\mathrm{B}(\\alpha ,\\beta )$ is there for the density to integrate to 1, don't worry too much about it).\n",
        "\n",
        "- now we combine them using the Bayes formula:\n",
        "$$ p(\\theta|data) = \\frac{p(data|\\theta) p(\\theta)}{p(data)} $$\n",
        "\n",
        "$$ = \\binom {n_{heads} + n_{tails}}{n_{heads}} \\frac{\\theta^{n_{heads}} (1 - \\theta)^{n_{tails}} \\theta^{\\alpha -1}(1-\\theta)^{\\beta -1}}{\\mathrm{B}(\\alpha ,\\beta ) p(data)} $$\n",
        "\n",
        "$$ = \\frac{\\theta^{n_{heads} + \\alpha -1} (1 - \\theta)^{n_{tails} + \\beta -1 }}{\\text{something that doesn't depend on } \\theta} $$\n",
        "\n",
        "$$‚àù \\theta^{n_{heads} + \\alpha -1} (1 - \\theta)^{n_{tails} + \\beta -1 }$$\n",
        "\n",
        "The $‚àù$ symbol means \"proportional to\" and here the trick is to see that we don't really care about things that are not related to $\\theta$. In other words it's already a lot to know the posterior density up to a constant, we can deal with that (using MCMC for instance). For know just trust me and assume that it's enough :)\n",
        "\n",
        "In this simple example we can simply \"recognize\" the distribution: it's a $Beta(n_{heads} + \\alpha, n_{tails} + \\beta)$. So easy! It's a particular case where the prior and the posterior are in the same family (we say they are \"conjugate\").\n",
        "\n",
        "The posterior mean is\n",
        "$$ \\frac{n_{heads} + \\alpha}{n_{heads} + n_{tails} + \\beta + \\alpha}$$\n",
        "\n",
        "($\\alpha$ and $\\beta$ are in some sense equivalent to some pseudo data points: $\\alpha$ extra heads and $\\beta$ extra tails)."
      ],
      "metadata": {
        "id": "R_Qa1YFpYdJt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's say I chose $\\alpha = 1$ and $\\beta = 1$ to set my prior (it's a uniform distribution actually, just check). I can now plot my posterior distribution:"
      ],
      "metadata": {
        "id": "h09yXjVOdl_A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import beta, binom\n",
        "\n",
        "alpha_ =  1\n",
        "beta_ = 1\n",
        "n_heads, n_tails = 9, 1\n",
        "likelihood = lambda theta: binom.pmf(n_heads, n_heads + n_tails, theta)\n",
        "\n",
        "x = np.arange (0.01, 1, 0.01)\n",
        "\n",
        "f, ax = plt.subplots(1, 1, figsize=(15, 4))\n",
        "plt.plot(x, beta.pdf(x, alpha_, beta_), label=\"Prior density\")\n",
        "plt.plot(x, likelihood(x), label=\"likelihood\")\n",
        "plt.plot(x, beta.pdf(x, n_heads + alpha_, n_tails + beta_), label=\"Posterior density\")\n",
        "plt.title(\"Coin flip example: uniform prior\")\n",
        "plt.xlabel(\"theta\")\n",
        "plt.axvline(x=n_heads / (n_heads + n_tails), label=\"Frequentist estimate\", linestyle=\"--\", color=\"orange\")\n",
        "plt.axvline(x=(n_heads + alpha_) / (n_heads + n_tails + alpha_ + beta_), label=\"Posterior mean\", linestyle=\"--\", color=\"green\")\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "A0t-eBQagpyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we look at the posterior: taking the mode (peak) of the posterior as an estimator raises the same result as the maximum likelihood / frequentist estimate but taking the posterior mean we are shifting a bit to the left: <font color='orange'>the prior regularizes a bit the estimate towards 0.5</font>. But the interest of Bayesian inference is not to get back to single point wise estimates but to <font color='orange'>embrace the full posterior distribution</font>: look at its shape and take a moment to realize we went from a single vertical line to this nice curve."
      ],
      "metadata": {
        "id": "ldYGDLollRV5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "But let's be more opiniated: what if we have strong prior knowledge about the coin being a fair coin. We can update our prior to reflect that: we need a prior that puts more weight at the parameter values we think are more likely (here 0.5):"
      ],
      "metadata": {
        "id": "VTNx6OeBlxiM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "alpha_ =  30\n",
        "beta_ = 30\n",
        "n_heads, n_tails = 9, 1\n",
        "likelihood = lambda theta: binom.pmf(n_heads, n_heads + n_tails, theta)\n",
        "\n",
        "x = np.arange (0.01, 1, 0.01)\n",
        "\n",
        "f, ax = plt.subplots(1, 1, figsize=(15, 4))\n",
        "plt.plot(x, beta.pdf(x, alpha_, beta_), label=\"Prior density\")\n",
        "plt.plot(x, likelihood(x), label=\"likelihood\")\n",
        "plt.plot(x, beta.pdf(x, n_heads + alpha_, n_tails + beta_), label=\"Posterior density\")\n",
        "plt.title(\"Coin flip example: informative prior\")\n",
        "plt.xlabel(\"theta\")\n",
        "plt.axvline(x=n_heads / (n_heads + n_tails), label=\"Frequentist estimate\", linestyle=\"--\", color=\"orange\")\n",
        "plt.axvline(x=(n_heads + alpha_) / (n_heads + n_tails + alpha_ + beta_), label=\"Posterior mean\", linestyle=\"--\", color=\"green\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TQeIfRyyiVfn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And we see the posterior distribution is impacted a lot because of our (very informative) prior distribution."
      ],
      "metadata": {
        "id": "6GFK73bumlC9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PART I: What's the probability of a successul rocket launch?"
      ],
      "metadata": {
        "id": "NKmNdGYNRK2e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now apply all that to our initial question: rocket launches üöÄ"
      ],
      "metadata": {
        "id": "gD6UPrt5nAnw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A simple case: 3 rocket launches"
      ],
      "metadata": {
        "id": "pPFQ8-F3cngw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " We will consider predicting the success/failure for the next launch of the **Delta 7925H rocket**, which as of November 2020 has been **launched 3 times**, with 0 failed launches. Maximum likelihood would therefore give an estimate of 0% for the chance of failure. Obviously we will not use this crazy estimate today and instead use Bayesian methods.\n",
        "\n",
        "Let us start with the simplest Bayesian model for this task: we assume the **three launches are independent**, biased coin flips, all with a shared probability of failure (bias) given by an unknown parameter failureProbability. In other words, we assume a binomial likelihood. We also assume a beta prior on the failureProbability, the parameter p of the binomial (the parameter n is known and set to the number of observed launches, numberOfLaunches).\n"
      ],
      "metadata": {
        "id": "93S7TgANO0Bt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now code this in Numpyro to carry the estimation:"
      ],
      "metadata": {
        "id": "kA6VcaYQPTfD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lpOC2LJEUSi5"
      },
      "outputs": [],
      "source": [
        "def model(n_launches: int, n_failures: int):\n",
        "    failure_probability = numpyro.sample(\"failure_probability\", dist.Beta(1, 1))\n",
        "    return numpyro.sample(\"failures\", dist.Binomial(n_launches, failure_probability), obs=n_failures)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mcmc = MCMC(NUTS(model), num_warmup=500, num_samples=3000)\n",
        "mcmc.run(jax.random.PRNGKey(0), n_launches=3, n_failures=0)\n",
        "posterior_samples = mcmc.get_samples()"
      ],
      "metadata": {
        "id": "uYSXMeJdQajB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some explanations about the above code:\n",
        "- in Numpyro we defined a \"model\" function that takes as input some observed data (n_launches and n_failures)\n",
        "- inside the function we write the probabilistic model: random variables are declared with `numpyro.sample(name, distribution)` where we specify its name and the distribution it follows.\n",
        "- if we observe something, we add the `obs=` parameter to specify which data were observed for this variable and we condition the posterior probability on that."
      ],
      "metadata": {
        "id": "gZgI4Vscflgp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The MCMC block conducts the inference: we won't explain it in details here but the overall idea is that since the model might be complex, we might not be able to compute the posterior distribution analytically so we need to approximate it using this MCMC scheme: it builds a Markov Chain of values that should converge to an equilibrium state where our posterior distribution can be approximated by samples from the chain."
      ],
      "metadata": {
        "id": "eC4W6_nsgXQ5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In practice: Numpyro handles that for us and <font color='orange'>we just care about the posterior samples we retrieve: those are our results</font> (for real one should check the chains are mixing well etc. but this is only a tutorial...)."
      ],
      "metadata": {
        "id": "cZ2sWkpLgvaY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "posterior_samples.keys()"
      ],
      "metadata": {
        "id": "n7EU_Fo2RiBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can look at the estimated parameter:"
      ],
      "metadata": {
        "id": "Kbl5_lvyeoYa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mcmc.print_summary()"
      ],
      "metadata": {
        "id": "x-JEd-0gR4fw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that our confidence interval is [0, 0.43] for the failure rate and a median estimate would be 16% chances of failure."
      ],
      "metadata": {
        "id": "bJCnZmvbg7CK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's also look at the posterior samples (as an histogram on the left, as the MCMC chain on the right):"
      ],
      "metadata": {
        "id": "j_B2y-qOenzV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_trace(mcmc)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "l3Bnv5YoecgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The distribution has more mass near 0 but it's pretty wide: this reflects our uncertainty about the estimate because we have only 3 observations."
      ],
      "metadata": {
        "id": "OZA0QrdwfVur"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The main objection: prior sensitivity"
      ],
      "metadata": {
        "id": "db2dymYzhQgM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To motivate the need for hierarchical models, we will now use our rocket example to demonstrate that the **posterior distribution is sensitive to the choice of prior when the dataset is small**.\n",
        "\n",
        "But before doing this, we will transform the parameters of the beta prior ($\\alpha$ and $\\beta$) into a new pair of parameters ($\\mu$ and $s$) which are more interpretable. The transformation we use is:\n",
        "$$ \\mu = \\frac{\\alpha}{\\alpha + \\beta} $$ and $$s = \\alpha + \\beta$$\n",
        "\n",
        "where $\\mu \\in (0, 1)$ and $s > 0$, which is invertible with an inverse given by\n",
        "$$\\alpha = \\mu s$$ and $$\\beta = (1 - \\mu) s$$\n",
        "\n",
        "The interpretation of this reparameterization is:\n",
        "\n",
        "$\\mu$: the mean of the prior distribution,\n",
        "\n",
        "$s$: a measure of \"peakiness\" of the prior density; higher $s$ correspond to a more peaked density; roughly, $s$ ‚âà number of data point (\"pseudo-observations\") that would make the posterior peaked like that.\n",
        "\n",
        "Here is the reparameterized code, which gives the same approximation as before (\n",
        "$\\mu$ is labelled mu in the code):"
      ],
      "metadata": {
        "id": "Bmy0edj8hWI8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model_reparameterized(n_launches: int, n_failures: int):\n",
        "    mu = 0.5\n",
        "    s = 2\n",
        "    failure_probability = numpyro.sample(\"failure_probability\", dist.Beta(mu * s, (1 - mu) * s))\n",
        "    return numpyro.sample(\"failures\", dist.Binomial(n_launches, failure_probability), obs=n_failures)"
      ],
      "metadata": {
        "id": "Qbo991eSfgu6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mcmc = MCMC(NUTS(model_reparameterized), num_warmup=500, num_samples=3000)\n",
        "mcmc.run(jax.random.PRNGKey(0), n_launches=3, n_failures=0)\n",
        "posterior_samples = mcmc.get_samples()"
      ],
      "metadata": {
        "id": "La9a7pSejAlL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mcmc.print_summary()"
      ],
      "metadata": {
        "id": "QkR66yoFjCpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_trace(mcmc)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lfvcQBffjEsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "üñä EXERCISE 1\n",
        "\n",
        "Change the prior mean parameter, e.g. from the \"neutral\" value of\n",
        "0.5 to a more \"optimistic\" value of 0.1, to demonstrate sensitivity of the posterior to the choice of prior. Report the change in the credible intervals, which should shrink considerably when moving to the optimistic prior.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "N_uUjb97jWak"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Side data"
      ],
      "metadata": {
        "id": "EYak95Fsjz0Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "How to attenuate prior sensitivity? The key idea that will lead us to hierarchical models is to **use side data to inform the prior**. Back to the rocket application, recall that we are interested in one type of rocket called Delta 7925H. In this context, side data takes the form of success/fail launch data from other types of rockets.\n",
        "\n",
        "Here is the data that we will use (showing only the first 10 rows out of 334 rows):"
      ],
      "metadata": {
        "id": "3F_JoaMFj1J4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"https://raw.githubusercontent.com/ctiennot/bayesian_tutorial/main/data/counts.csv\"\n",
        "df_launches = pd.read_csv(path, index_col=0)\n",
        "df_launches.head(10)"
      ],
      "metadata": {
        "id": "3c9mLD3YjzAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Non Bayesian use of the data"
      ],
      "metadata": {
        "id": "54cnybrcp1A7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we describe the Bayesian way to use side data, let us briefly go over a more naive method. For now, let us focus on the prior parameter $\\mu$.\n",
        "\n",
        "Here is the naive method to determine the prior parameter $\\mu$ from our \"side data\":\n",
        "\n",
        "Estimate the failure probability $p_i$ for each rocket type $i$ (e.g. using maximum likelihood estimation, for binomial this is just the fraction of rocket failures for each rocket type). Let us denote each estimate by $\\hat p_i$\n",
        "\n",
        "Fit a distribution over all the estimated failure probabilities\n",
        "$\\hat p_i$'s\n",
        "\n",
        "Use this fit distribution as the prior on $\\mu$."
      ],
      "metadata": {
        "id": "ugv1cfJVp3V9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "estimated_p_i_s = df_launches[\"numberOfFailures\"] / df_launches[\"numberOfLaunches\"]\n",
        "\n",
        "plt.figure(figsize=(15, 4))\n",
        "plt.hist(estimated_p_i_s, bins=30)\n",
        "plt.xlabel(\"$\\hat p_i$ (estimated failure rate per rocket types)\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.title(\"The ugly way to build a prior distribution\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rRo_qPfWpvGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "This method provides some intuition on how we can use side data to inform our choice of prior distribution. However this naive method has some weaknesses. A first weakness is that it is less obvious how to proceed with the precision parameter $s$. A second weakness is that the distribution estimated in step 2 above has some irregularities, visible in the histogram of MLE-estimated\n",
        "$\\hat p_i$'s in our dataset"
      ],
      "metadata": {
        "id": "tg9ckEKkqWky"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "üñä EXERCISE 2\n",
        "\n",
        "Notice the bumps at 1/2 and 1 in the above histogram. Speculate on the cause of these two bumps or irregularities in the histogram.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "Wz14g97jrwa4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bayesian use of side data: the Hierarchical Model"
      ],
      "metadata": {
        "id": "aHZOz-Vvr-T7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"center\"><img src=\"https://github.com/ctiennot/bayesian_tutorial/blob/main/pictures/hierarchical-gm.jpg?raw=true\" width=\"700\"/></p>"
      ],
      "metadata": {
        "id": "T7rm1ItwtS8_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A general principle in Bayesian inference is to treat **unknown quantities as random**. This is also the essence of a Bayesian hierarchical model: let us treat $\\mu$ and $s$ as random and let the side data inform their posterior distribution.\n",
        "\n",
        "To visualize what a hierarchical model is, let us look at the corresponding graphical model, shown here. The circles are random variables, the squares, non-random. Shaded nodes are observed, white ones, unknown. An arrow from a variable $X$ to $Y$ means that the generative model for $Y$ has a parameter that depends on $X$. The rectangle enclosing the bottom three nodes is called a plate, encoding the fact that the \"plated\" random variables (those inside the rectangle) are copied, once for each rocket type (i.e. for each row in the data table shown earlier).\n",
        "\n",
        "The model inside the plate is just several copies of the BasicReparameterized model. You can think of this as the \"bottom level\" of a hierarchy. What we have added in the hiearchical model is the \"top level\", which consists of two random variables, one for $\\mu$, and one for $s$, each with its own prior distribution. Here we will use a beta prior for $\\mu$ and an exponential prior for $s$."
      ],
      "metadata": {
        "id": "OgdQJRfksEsM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's implement this in Numpyro:"
      ],
      "metadata": {
        "id": "CRKZBy52R1Lz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def hierarchical_model_slow(n_launches: List[int], n_failures: List[int], rocket_type_names: List[str]):\n",
        "    assert len(n_launches) == len(n_failures) == len(rocket_type_names)\n",
        "    n_rocket_types = len(rocket_type_names)\n",
        "\n",
        "    # prior distribution parameters\n",
        "    mu_mean, mu_s = 0.5, 2\n",
        "\n",
        "    # prior distributions for the common failure probabilities distribution\n",
        "    mu = numpyro.sample(\"mu\", dist.Beta(mu_mean * mu_s, (1 - mu_mean) * mu_s)) # mu has become a random variable!\n",
        "    s = numpyro.sample(\"s\", dist.Exponential(0.1)) # s has become a random variable!\n",
        "\n",
        "    for i in range(n_rocket_types):  # it would be more efficient to vectorize here but it's easier to read for the tutorial...\n",
        "      rocket_type = rocket_type_names[i]\n",
        "      failure_probability = numpyro.sample(f\"failure_probability_{rocket_type}\", dist.Beta(mu, s)) # a failure probability for this rocket type is sampled from the common distribution of failure probabilities\n",
        "      return numpyro.sample(f\"failures_{rocket_type}\", dist.Binomial(n_launches[i], failure_probability), obs=n_failures[i]) # we observe the failures for this rocket type"
      ],
      "metadata": {
        "id": "mhAsDQDbRZT1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Turns out the for loop is super slow in practice and we will vectorize things to go faster :)"
      ],
      "metadata": {
        "id": "7v1WZuvjVtFs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def hierarchical_model(n_launches: List[int], n_failures: List[int], rocket_type_names: List[str]):\n",
        "    assert len(n_launches) == len(n_failures) == len(rocket_type_names)\n",
        "    n_rocket_types = len(rocket_type_names)\n",
        "\n",
        "    # prior distribution parameters\n",
        "    mu_mean, mu_s = 0.5, 2\n",
        "\n",
        "    # prior distributions for the common failure probabilities distribution\n",
        "    mu = numpyro.sample(\"mu\", dist.Beta(mu_mean * mu_s, (1 - mu_mean) * mu_s)) # mu has become a random variable!\n",
        "    s = numpyro.sample(\"s\", dist.Exponential(0.1)) # s has become a random variable!\n",
        "\n",
        "    # Numpyro can vectorize operations using this plate mechanism :)\n",
        "    with numpyro.plate(\"rocket_types\", n_rocket_types):\n",
        "      failure_probabilities = numpyro.sample(f\"failure_probabilities\", dist.Beta(mu * s, (1 - mu) * s))\n",
        "      return numpyro.sample(f\"failures\", dist.Binomial(n_launches, failure_probabilities), obs=n_failures)"
      ],
      "metadata": {
        "id": "NpI1FImyVsXQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some explanations of the above code:\n",
        "\n",
        "- the \"plate\" statement correspond to the to the rectangle on the graph above.\n",
        "- `mean_mu` and `mu_s` are needed to parametrize the $\\mu$ distribution\n",
        "- the $s$ distribution is also parametrize using 0.1 in the exponential\n"
      ],
      "metadata": {
        "id": "DNWzD_zaVPEQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we'll run the MCMC chain:"
      ],
      "metadata": {
        "id": "SN_vWUFWUneG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mcmc = MCMC(NUTS(hierarchical_model), num_warmup=500, num_samples=3000)\n",
        "mcmc.run(\n",
        "    jax.random.PRNGKey(0),\n",
        "    n_launches=df_launches[\"numberOfLaunches\"].values,\n",
        "    n_failures=df_launches[\"numberOfFailures\"].values,\n",
        "    rocket_type_names=df_launches[\"rocketType\"].values\n",
        "  )\n",
        "posterior_samples = mcmc.get_samples()"
      ],
      "metadata": {
        "id": "boUHq_o2TxGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mcmc.print_summary()"
      ],
      "metadata": {
        "id": "OE41r6fEXNQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# can take several minutes to plot!\n",
        "plot_trace(mcmc)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9vIFuIv2X6hE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now have a lot of parameters in the MCMC summary üòÆ Because of the plate statement we have one node / random variable per failure probability, so per rocket type (and we have 335 of them). But look at the $\\mu$ estimate: we now have a 0.12 median and [0.10, 0.13] as our (90%) credible interval."
      ],
      "metadata": {
        "id": "OFaueYehX8Uw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the new model still has prior distributions at the top level, and hence parameters to set for these new top level priors. Are we back to square one? No, as **it turns out these top-level parameters will be generally less sensitive** as you will verify in the following exercise."
      ],
      "metadata": {
        "id": "8OIwYy0UVUQ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "üñä EXERCISE 3\n",
        "\n",
        "Change the prior mean on the $\\mu$ variable, e.g. from the \"neutral\" value of 0.5 to a more \"optimistic\" value of 0.1, to demonstrate decreased sensitivity of the posterior to the prior choice in the hierarchical model compared to the basic model. Report the change in the credible intervals for Delta 7925H's failure probability, which should be almost the same when moving to an optimistic prior.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "_8hjpqXeVdO_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Understanding what just happened"
      ],
      "metadata": {
        "id": "r_x0_oOEaGsH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "How to explain this reduced sensitivity? One heuristic is based on the graphical model. For each unobserved (white) node in the graphical model, the number of edges connected to that node is often indicative of how peaky the posterior distribution is, and peaky posteriors tend to be less sensitive to prior choice.\n",
        "\n",
        "Consider for example the graphical model for the basic model (left below). Note that the variable $p$ (failure probability for the Delta 7925H) has only 3 edges connected to it. Even if we expanded the observed number of failures (F) to a list of Bernoulli random variables, this would only lead to two more connections. In contrast, with the hierarchical model (right), the top nodes have 334 connections (the number of rocket types, i.e. rows in the data table). The theorem behind this vague heuristic is the [Bernstein‚Äìvon Mises theorem](https://en.wikipedia.org/wiki/Bernstein%E2%80%93von_Mises_theorem)."
      ],
      "metadata": {
        "id": "7Aex2jnoaBF8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"center\">\n",
        "<img src=\"https://www.stat.ubc.ca/~bouchard/courses/stat520-sp2021-22/figs/hierarchical-gm-before.jpg\" height=\"300\"/>\n",
        "<img src=\"https://www.stat.ubc.ca/~bouchard/courses/stat520-sp2021-22/figs/hierarchical-gm-after.jpg\" height=\"300\"/>\n",
        "</p>"
      ],
      "metadata": {
        "id": "RmgBVtj2aSWz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting back to the BBC article"
      ],
      "metadata": {
        "id": "JkzLtoUdczZO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So is the claim in the BBC article false? We did find a 10% to 13% to be a 90% credible interval for our \"global\" failure rate which doesn't include the 5% figure stated in the article.\n",
        "\n",
        "Well let's first state a few things:\n",
        "- our dataset stops at Nov 2020\n",
        "- the data go back up to 1957 and it's clear that there have been improvement and probably the failure rate has decreased in time\n",
        "- all launchers are not equal (hence the groups in our Bayesian model)"
      ],
      "metadata": {
        "id": "oN3Gud06c2LC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The article states:\n",
        ">\"Last year (2022), there were 186 launches,\" he says. These carried 2,509 satellites into orbit, the majority of those being Starlink, SpaceX's satellite internet constellation. \"\n",
        "\n",
        "If we take the Starlink launches, most of those are using the Falcon 9  launch vehicle.\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://github.com/ctiennot/bayesian_tutorial/blob/main/pictures/falcon9_screenshot.jpg?raw=true\" height=\"300\"/>\n",
        "</p>"
      ],
      "metadata": {
        "id": "d7ycxqIEereT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_launches.loc[df_launches[\"rocketType\"].str.contains(\"Falcon\")]"
      ],
      "metadata": {
        "id": "lbl7OFEuVRtJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "falcon9_i = list(df_launches[\"rocketType\"].values).index(\"Falcon 9\")\n",
        "falcon9_i"
      ],
      "metadata": {
        "id": "BEtDyWH4dla3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "falcon_9_post_samples = posterior_samples[\"failure_probabilities\"][:, falcon9_i]\n",
        "median = np.mean(falcon_9_post_samples)\n",
        "ci = np.percentile(falcon_9_post_samples, (2.5, 97.5))\n",
        "\n",
        "plt.figure(figsize=(15, 4))\n",
        "plt.hist(falcon_9_post_samples, bins=50)\n",
        "plt.title(\"Flacon 9 failure rate estimate\")\n",
        "plt.xlabel(\"Failure rate\")\n",
        "plt.axvline(median, linestyle=\"--\", color=\"red\", label=\"Median\")\n",
        "plt.text(median + 0.001, 200, f\"Median = {median:.2%}\", color=\"red\")\n",
        "plt.axvspan(*ci, facecolor='red', alpha=0.1, label=\"95% CI\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5elcuN6SfigW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So the BBC estimate might not be wrong, but it's vague regarding what type of rockets and we can blame them for not giving confidence interval. Bad student."
      ],
      "metadata": {
        "id": "-jMtkw_fJM1Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "If you got to this part of the tutorial, good game, that's already great! I initially planned to stop there (the original tutorial did) but then I thought \"what if some folks finish too early\"?\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://www.shutterstock.com/image-photo/funny-cocktail-dog-holding-martini-260nw-132475163.jpg\" height=\"300\"/>\n",
        "</p>"
      ],
      "metadata": {
        "id": "j2uaIA7HIVWn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feel free to stop there if you're bored or go have a quick coffe and/or chat with your colleagues before keeping the hard work."
      ],
      "metadata": {
        "id": "DOZeA4K9J1cQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PART II: More fact checking, more models, more fun"
      ],
      "metadata": {
        "id": "GWjsXgpQRV7Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Is Wade right? A good old logistic regression to the rescue"
      ],
      "metadata": {
        "id": "vw2cRpGKIQP1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We've also read in the BBC article:\n",
        ">\"Typically, first or second launch, you expect something like 30% of them to fail,\" says Wade. \"Then things start to get better thereafter, by the time you're up to the 10th flight, you're probably looking at a less than 5% failure rate.\"\n",
        "\n",
        "And we want to double check what Wade is saying. Let's load some more detailed data (one row per launch instead of agregated figures):"
      ],
      "metadata": {
        "id": "uxvF9QefIT_k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_detailed = pd.read_csv(\"https://raw.githubusercontent.com/ctiennot/bayesian_tutorial/main/data/processed.csv\", index_col=0)\n",
        "df_detailed.head(10)"
      ],
      "metadata": {
        "id": "aEP_NTYkK56o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can plot a few descriptive statistics, for instance the number of launches through time:"
      ],
      "metadata": {
        "id": "zgtOL-NGD5hA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15, 4))\n",
        "df_detailed[\"year\"].value_counts().sort_index().plot.bar()\n",
        "plt.title(\"Launches through time\")\n",
        "plt.xlabel(\"Year\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CCJwNspvLL2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Or the counts per rocket types:"
      ],
      "metadata": {
        "id": "1FycgMoaEBTA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15, 6))\n",
        "df_detailed[\"rocketType\"].value_counts().head(20).iloc[::-1].plot.barh()\n",
        "plt.title(\"Top Rocket types\")\n",
        "plt.xlabel(\"Number of launches\")\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rjcPQHEeMmst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The \"launchIndex\" variable tells us the index of the launch for the given\n",
        "\"rocketType\". So `launchIndex = 1` means it's the first time this rocket type was tried. So could we simply groupby this field and check the failure rate?"
      ],
      "metadata": {
        "id": "pBp25eX0NyZV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_detailed[\"first_rocket_type_launch\"] = df_detailed[\"launchIndex\"] == 1"
      ],
      "metadata": {
        "id": "hcOn3m3EOuTO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll report the Confidence Intervals (CI) in a frequentist way (using a normal approx)."
      ],
      "metadata": {
        "id": "tv2nP0_KELXU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_bernouilli_CI_width(means, counts):\n",
        "  # normal approx, centered on means\n",
        "  return 1.96 * np.sqrt(means * (1 - means) / counts)\n",
        "\n",
        "success_per_launch_index = df_detailed.groupby(\"launchIndex\").agg({'success':['mean', 'std', 'count']})\n",
        "success_per_launch_index[\"ci_width\"] = compute_bernouilli_CI_width(success_per_launch_index[('success',  'mean')], success_per_launch_index[('success',  'count')])\n",
        "success_per_launch_index.head(5)"
      ],
      "metadata": {
        "id": "TifWAFoUNwBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's plot those by the launch index to see what it gives:"
      ],
      "metadata": {
        "id": "bippb3UuEYnP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_to_display = 20\n",
        "plt.figure(figsize=(15, 4))\n",
        "success_per_launch_index.head(n_to_display)[('success',  'mean')].plot.bar(yerr=success_per_launch_index[\"ci_width\"].head(n_to_display))\n",
        "plt.xlabel(\"Launch Index\")\n",
        "plt.ylabel(\"Estimated success rate\")\n",
        "plt.title(\"Is the first Launch that unsuccessful?\")\n",
        "plt.ylim((0.6, 1))\n",
        "plt.grid()\n",
        "plt.axhline(y=0.7, color=\"red\", linestyle=\"--\")\n",
        "t = plt.text(0, 0.72, \"Wade's first launch success rate estimate\", color=\"red\")\n",
        "t.set_bbox(dict(facecolor='white', alpha=0.6, edgecolor=\"white\"))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WDzcnzv0OM0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This (frequentist) simple analysis would prove Wade estimate to be wrong: the success rate would be more around 83% for first launches (i.e. 17% failure rate, not 30%).\n",
        "\n",
        "But aren't there any bias here? The success might also depend of the year (systems reliability might improve) of the country, of the rocketType (as in first part) etc. We should account for those.\n",
        "\n",
        "A natural way to do so is by **fitting a logistic regression to predict the success / failure of a rocket launch**. That way if we put the right variables in there we will be able to control for those biases. We could use the sklearn one but it won't report confidence intervals and other \"stats\" things so we'll use the statsmodel one."
      ],
      "metadata": {
        "id": "crUfjjVVlU0G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = (pd.get_dummies(df_detailed[[\"year\", \"first_rocket_type_launch\", \"country\"]]) * 1).astype(float)\n",
        "X[\"year_after_1956\"] = X[\"year\"] - 1956\n",
        "X.drop(columns=[\"year\"], inplace=True)\n",
        "X.drop(columns=[\"country_USA\"], inplace=True) # USA will be our reference country\n",
        "y = df_detailed[\"success\"]"
      ],
      "metadata": {
        "id": "SdS1lRTvnppz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.head()"
      ],
      "metadata": {
        "id": "_nA9oywWqlax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logit_mod = sm.Logit(y, sm.add_constant(X))"
      ],
      "metadata": {
        "id": "0dRLAns2mf0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logit_res = logit_mod.fit()"
      ],
      "metadata": {
        "id": "5NabuzWKowck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(logit_res.summary())"
      ],
      "metadata": {
        "id": "9bou2QjUrz3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before jumping to conclusions, let's check if the fit is not too poor. For linear regression we would check the residuals for instance to see if the model fit weel the data. Here for a logistic regression a common way is to split the predicted probabilities in bins / buckets and see if the empirical output rate matches the predicted one on the bucket. This is called **checking \"model calibration\"**."
      ],
      "metadata": {
        "id": "kvDfSosFHRzk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.calibration import calibration_curve\n",
        "\n",
        "def plot_calibration_curve(logit_res, X, y):\n",
        "  prob_true, prob_pred = calibration_curve(y, logit_res.predict(sm.add_constant(X)), n_bins=10, strategy=\"quantile\")\n",
        "\n",
        "  f = plt.figure(figsize=(10, 4))\n",
        "  plt.plot(prob_pred, prob_true, \"o-\")\n",
        "  plt.plot([0.7, 1], [0.7, 1], linestyle=\"--\")\n",
        "  plt.xlabel(\"Predicted proba\")\n",
        "  plt.ylabel(\"Real proba in bucket\")\n",
        "  plt.title(\"Calibration plot for the Logistic regression\")\n",
        "  plt.show()\n",
        "\n",
        "plot_calibration_curve(logit_res, X, y)"
      ],
      "metadata": {
        "id": "-4nOkPz4yUJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll see we can do better but let's keep up for now."
      ],
      "metadata": {
        "id": "LHPQXHgWFc0X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parameters are living in the logit space and are not super interpretable but we can compute <font color='orange'>marginal effects</font> of those: put simply \"how does a unit increase of the variable impact the output variable\"."
      ],
      "metadata": {
        "id": "pNiwQQ4ynOCO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(logit_res.get_margeff().summary())"
      ],
      "metadata": {
        "id": "yUN-naWft62o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- a first launch decreases by <font color='orange'>6 points of percentage</font> the success probability of a successful launch.\n",
        "- in average a China's rocket launch success probability is decreased by 6.2 percent points compared to a USA one. Europe and Russia effects are not significant.\n",
        "- in average each year that past since 1957 increased by 0.28 points of percentage the success probability of launching rockets."
      ],
      "metadata": {
        "id": "z09r_o7xn-0s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "But that's a marginal effect, what about concrete probabilities? Let's See for a US rocket through the years what it gives:"
      ],
      "metadata": {
        "id": "NKQuUvGoGd6G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fitted_params = logit_res.params\n",
        "for year in [1960, 1970, 1980, 1990, 2000, 2010, 2020]:\n",
        "  not_first_time_estimate = 1 / (1 + np.exp(-(fitted_params[\"const\"] + fitted_params[\"year_after_1956\"] * (year - 1956))))\n",
        "  first_time_estimate = 1 / (1 + np.exp(-(fitted_params[\"const\"] + fitted_params[\"first_rocket_type_launch\"] + fitted_params[\"year_after_1956\"] * (year - 1956))))\n",
        "  print(f\"Success for a first time US launch in {year}: {first_time_estimate:.2%}\")\n",
        "  print(f\"Success for a non first time US launch in {year}: {not_first_time_estimate:.2%}\\n\")"
      ],
      "metadata": {
        "id": "9wTkBcHD57wP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The effect of the year seems huge. Also having a linear dependency on it is a bit weird, let's check what the data tell us:"
      ],
      "metadata": {
        "id": "dftUw3E9GcDC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_detailed.groupby(\"year\")[\"success\"].mean().apply(lambda p: np.log(p / (1 - p))).plot()\n",
        "plt.title(\"Avg success rate accross year (logit scale)\")\n",
        "plt.ylabel(\"Logit(empirale rate)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "K68kqU85HrKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that a linear coefficient on year doesn't really make sense, why not add a coefficient on the log of the year?"
      ],
      "metadata": {
        "id": "7Pi__-ntJxn2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X[\"log_year_after_1956\"] = np.log(X[\"year_after_1956\"])\n",
        "logit_mod = sm.Logit(y, sm.add_constant(X))\n",
        "logit_res = logit_mod.fit()\n",
        "print(logit_res.summary())\n",
        "plot_calibration_curve(logit_res, X, y)"
      ],
      "metadata": {
        "id": "EQMcLa3gNAhi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The added coefficient is statistically significant and the calibration curve is way better. Let's check again our probabilities:"
      ],
      "metadata": {
        "id": "eKxgEB6FN0RO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fitted_params = logit_res.params\n",
        "for year in [1960, 1970, 1980, 1990, 2000, 2010, 2020]:\n",
        "  common_logit_effect = fitted_params[\"const\"] + fitted_params[\"year_after_1956\"] * (year - 1956) + fitted_params[\"log_year_after_1956\"] * np.log(year - 1956)\n",
        "  not_first_time_estimate = 1 / (1 + np.exp(-common_logit_effect))\n",
        "  first_time_estimate = 1 / (1 + np.exp(-(common_logit_effect + fitted_params[\"first_rocket_type_launch\"])))\n",
        "  print(f\"Success for a first time US launch in {year}: {first_time_estimate:.2%}\")\n",
        "  print(f\"Success for a non first time US launch in {year}: {not_first_time_estimate:.2%}\\n\")"
      ],
      "metadata": {
        "id": "7phGHukROa3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interesting... There could be many more things to do there, but I'm already taking too much time to prepare this tutorial and I want to get to the Bayesian counterpart of the logistic regression so I'll leave you carry further analysis and improve the model if you want. What we can say is:\n",
        "> <font color='orange'>Given our data, and if we trust this basic model it seems a 30% failure rate for a (US) first launch is exagerated</font>.\n",
        "\n",
        "Let's be cautious because we don't have the recent year data and our model is not to be trusted blindly, we didn't carry in depth goodness of fit checks (execpt a quick calibration curve one)."
      ],
      "metadata": {
        "id": "ev8mhDIdPiA-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "üñä EXERCISE 4 (ideas)\n",
        "\n",
        "Try to improve this quick model / analysis:\n",
        "- what features could you add?\n",
        "- Can you spot an issue with the way we dealt with the \"year\" variable?\n",
        "- would it be a good idea to split our data in a train / test to assess the predictive power of our logistic regression? If so how would you split? What kind of AUC / f1 score do you except to get?\n",
        "- Could you think of something else than logistic regression? Maybe trees? What are the pros and cons of this?\n",
        "- We have many boolean variables in our logistic regression, what could we do about it? At what price does it come?\n",
        "\n",
        "And please give a shot at any other idea you might have.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "fbIp3XF0F-FE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bayesian logistic regression"
      ],
      "metadata": {
        "id": "AsHKJVXuRngk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now notice that in the above **we only got point estimates** of our probabilities like \"for a US rocket in 1990 launching for the first time the probability is 93.68%\" but **what if we want a confidence inteval on this**? The logistic regression outputs give us CI on coefficients but not on the predictions. This is not impossible to derive (see [this post](https://stats.stackexchange.com/questions/354098/calculating-confidence-intervals-for-a-logistic-regression) for instance) in the frequentist world but this looks tricky to me (feel free to disagree) for something so simple in the Bayesian world (at the cost of more compute power to be fair). And the Bayesian method comes with many other advantages, stay tuned.\n",
        "\n",
        "So let's see how we can fit the same logistic regression in a Bayesian way.\n"
      ],
      "metadata": {
        "id": "oceIBvEFRq1w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpyro\n",
        "import numpyro.distributions as dist\n",
        "from numpyro.infer import MCMC, NUTS\n",
        "import numpy as np\n",
        "import jax.numpy as jnp\n",
        "from jax import random\n",
        "\n",
        "\n",
        "def model(years, country_idx, is_first_launch, y=None):\n",
        "    # Prior for the intercept\n",
        "    intercept = numpyro.sample('intercept', dist.Normal(0, 20))\n",
        "\n",
        "    # Priors for the coefficients of years and log(years)\n",
        "    beta_years = numpyro.sample('beta_years', dist.Normal(0, 5))\n",
        "    beta_log_years = numpyro.sample('beta_log_years', dist.Normal(0, 5))\n",
        "\n",
        "    # Handling the categorical variable 'country_idx'\n",
        "    num_countries = len(np.unique(country_idx))\n",
        "    beta_country = numpyro.sample('beta_country', dist.Normal(jnp.zeros(num_countries), jnp.ones(num_countries)))\n",
        "\n",
        "    beta_first_launch = numpyro.sample('beta_first_launch', dist.Normal(0, 1))\n",
        "\n",
        "    # Linear combination of inputs (including log(years))\n",
        "    log_years = jnp.log(years)\n",
        "    logits = numpyro.deterministic(\n",
        "        \"logits\",\n",
        "        intercept + beta_years * years + beta_log_years * log_years + beta_country[country_idx] + is_first_launch * beta_first_launch\n",
        "    )\n",
        "    probas = numpyro.deterministic(\"probas\", jax.scipy.special.expit(logits))\n",
        "\n",
        "    with numpyro.plate('data', len(years)):\n",
        "        obs = numpyro.sample('obs', dist.Bernoulli(logits=logits), obs=y)\n",
        "\n",
        "\n",
        "def run_inference(model, years, country_idx, is_first_launch, y, rng_key, num_warmup=500, num_samples=1000):\n",
        "    # Running MCMC\n",
        "    kernel = NUTS(model)\n",
        "    mcmc = MCMC(kernel, num_warmup=num_warmup, num_samples=num_samples)\n",
        "    mcmc.run(rng_key, years, country_idx, is_first_launch, y)\n",
        "    return mcmc, mcmc.get_samples()\n",
        "\n",
        "\n",
        "\n",
        "countries = df_detailed[\"country\"].values\n",
        "# Convert categorical variables to integers\n",
        "_, country_idx = np.unique(countries, return_inverse=True)\n",
        "years = df_detailed[\"year\"].values - 1956\n",
        "is_first_launch = df_detailed[\"first_rocket_type_launch\"].values\n",
        "y = df_detailed[\"success\"].values\n",
        "\n",
        "# Run the model\n",
        "rng_key = random.PRNGKey(0)\n",
        "mcmc, samples = run_inference(model, years, country_idx, is_first_launch, y, rng_key)\n",
        "\n",
        "# Print summary or analyze samples..."
      ],
      "metadata": {
        "id": "obhIwx2FRpda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We just used simple normal priors on each coefficient, the model remains the same as before."
      ],
      "metadata": {
        "id": "viqodlnfIRBC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mcmc.print_summary()"
      ],
      "metadata": {
        "id": "QA3Vfef1Z1mw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_trace(mcmc, var_names=[\"intercept\", \"beta_years\", \"beta_first_launch\", \"beta_country\"])\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "b5Bc8nVlbIvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The coefficients look similar to those of the former logistic, but now we've got some posterior distributions on those."
      ],
      "metadata": {
        "id": "YC7CrP-wIFiS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "And to derive our credible intervals about each prediction, let's introduce a very important concept in Bayesian inference: [**the posterior predictive distribution**](https://en.wikipedia.org/wiki/Posterior_predictive_distribution).\n",
        "\n",
        "In a nutshell the idea is that the uncertainty about a prediction is coming from two things:\n",
        "- the **model itself** (in a linear regression setting for instance, we assume some random noise in the model)\n",
        "- the **uncertainty on the parameter estimates**: because we are not sure about the parameters, we need to propagate this uncertainty to our predictions which are using those estimates."
      ],
      "metadata": {
        "id": "DgqKrF7dIjpe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Or as Wikipedia states it:\n",
        "\n",
        ">It may seem tempting to plug in a single best estimate but this ignores uncertainty about it and because a source of uncertainty is ignored, the predictive distribution will be too narrow. Put another way, predictions of extreme values will have a lower probability than if the uncertainty in the parameters as given by their posterior distribution is accounted for.\n",
        "\n",
        "I don't want to insist on the math side, you can wander around and find explanations on the topic if you're interested."
      ],
      "metadata": {
        "id": "Le_qtbYKJbsp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Numpyro we have an helper \"Predictive\" function that does teh job: we only need to specify it the model and give it samples from our posterior distribution."
      ],
      "metadata": {
        "id": "nw0vxGAJKA-y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpyro.infer import Predictive\n",
        "predictive = Predictive(model, posterior_samples={k: v for k, v in samples.items() if k not in ['logits', 'probas']})\n",
        "predictions_training_set = predictive(jax.random.PRNGKey(10), years=years, country_idx=country_idx, is_first_launch=is_first_launch, y=y)"
      ],
      "metadata": {
        "id": "cf71T-d5cwF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's sanity check the calibration:"
      ],
      "metadata": {
        "id": "WxmKtzzgIbKj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.calibration import calibration_curve\n",
        "\n",
        "prob_true, prob_pred = calibration_curve(y, predictions_training_set[\"probas\"].mean(axis=0), n_bins=10, strategy=\"quantile\")\n",
        "\n",
        "\n",
        "f = plt.figure(figsize=(10, 4))\n",
        "plt.plot(prob_pred, prob_true, \"o-\")\n",
        "plt.plot([0.7, 1], [0.7, 1], linestyle=\"--\")\n",
        "plt.xlabel(\"Predicted proba\")\n",
        "plt.ylabel(\"Real proba in bucket\")\n",
        "plt.title(\"Calibration plot for the Bayesian Logistic regression\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8VNRhQWH7VkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now plot a random sample of rocket launches with their CI displayed:"
      ],
      "metadata": {
        "id": "X12het5IKLnQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpyro.diagnostics import hpdi\n",
        "\n",
        "np.random.seed(1804)\n",
        "sample_obs = np.random.choice(range(len(y)), 50)\n",
        "pred_mean = predictions_training_set[\"probas\"].mean(axis=0)[sample_obs]\n",
        "pred_hpdi = hpdi(predictions_training_set[\"probas\"], 0.95)[:, sample_obs]\n",
        "\n",
        "order = np.argsort(pred_mean)"
      ],
      "metadata": {
        "id": "8EkezD7G7v4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(15, 7))\n",
        "\n",
        "plt.errorbar(\n",
        "    pred_mean,\n",
        "    range(len(sample_obs)),\n",
        "    xerr=pred_hpdi[1, :] - pred_mean,\n",
        "    marker=\"o\",\n",
        "    ms=5,\n",
        "    mew=4,\n",
        "    ls=\"none\",\n",
        "    alpha=0.8,\n",
        ")\n",
        "plt.xlabel(\"Estimated Success Probability\")\n",
        "plt.ylabel(\"Some observations sampled at random\")\n",
        "plt.title(\"Some random 95% credible intervals on predicted values\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TgZIA5sa95yV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And what about a US rocket in 2022?"
      ],
      "metadata": {
        "id": "vJfHojUGFFu4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "us_index = np.unique(country_idx[countries == \"USA\"])[0]"
      ],
      "metadata": {
        "id": "sjmvAqwVFT3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_us_2022_first_launch = predictive(jax.random.PRNGKey(10), years=np.array([2022 - 1956]), country_idx=np.array([us_index]), is_first_launch=np.array([True]), y=None)"
      ],
      "metadata": {
        "id": "ZJsbNu3ZFEza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15, 4))\n",
        "plt.hist(predictions_us_2022_first_launch[\"probas\"].flatten(), bins=30)\n",
        "plt.title(\"Estimated Success Probability for a US Rocket in 2022\")\n",
        "plt.xlabel(\"Probability of success\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zva2_B-kztCs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Can we check the evolution through time?"
      ],
      "metadata": {
        "id": "RwLhrsYqF_gJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "years_ = range(1960, 2022)\n",
        "predictions_us_through_time_first_launch = predictive(jax.random.PRNGKey(10), years=np.array([year - 1956 for year in years_]), country_idx=np.array([us_index]), is_first_launch=np.array([True]), y=None)[\"probas\"]\n",
        "predictions_us_through_time_not_first_launch = predictive(jax.random.PRNGKey(10), years=np.array([year - 1956 for year in years_]), country_idx=np.array([us_index]), is_first_launch=np.array([False]), y=None)[\"probas\"]"
      ],
      "metadata": {
        "id": "edTM814aFp_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15, 4))\n",
        "plt.plot(years_, predictions_us_through_time_first_launch.mean(axis=0), label=\"At first launch\")\n",
        "plt.fill_between(\n",
        "  years_,\n",
        "  np.percentile(predictions_us_through_time_first_launch, 5, axis=0),\n",
        "  np.percentile(predictions_us_through_time_first_launch, 95, axis=0),\n",
        "  alpha=0.2\n",
        ")\n",
        "plt.plot(years_, predictions_us_through_time_not_first_launch.mean(axis=0), label=\"After first launch\")\n",
        "plt.fill_between(\n",
        "  years_,\n",
        "  np.percentile(predictions_us_through_time_not_first_launch, 5, axis=0),\n",
        "  np.percentile(predictions_us_through_time_not_first_launch, 95, axis=0),\n",
        "  alpha=0.2\n",
        ")\n",
        "plt.title(\"Estimated of a Successful Launch Probability for a US Rocket\")\n",
        "plt.ylabel(\"Probability of success\")\n",
        "plt.xlabel(\"Year\")\n",
        "plt.grid()\n",
        "plt.legend(loc=4)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9KcWpoJ7FthI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "üñä EXERCISE 5\n",
        "\n",
        "You are now a Bayesian master, carve your own path.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "rdAq7Em4KbMr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"center\">\n",
        "<img src=\"https://i.redd.it/hwxab23r2r291.jpg\" height=\"300\"/>\n",
        "</p>"
      ],
      "metadata": {
        "id": "hZfv8b6DKzHf"
      }
    }
  ]
}